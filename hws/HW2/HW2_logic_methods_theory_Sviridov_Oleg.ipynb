{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G40l9lO2ObLr"
   },
   "source": [
    "<center><img src=\"./images/logo_fmkn.png\" width=300 style=\"display: inline-block;\"></center> \n",
    "\n",
    "## Домашнее задание №2, теоретическое. Логические методы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решите предложенные задачи. Каждая задача должна быть подробно обоснована, задачи без\n",
    "обоснования не засчитываются. Решения пишутся в свободной форме, однако так, чтобы проверяющий смог разобраться. Если проверяющий не смог разобраться в решении какой-нибудь задачи, то\n",
    "она автоматически не засчитывается. Если используются какие-либо внешние источники, их нужно обязательно указывать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C86UTDuUlQBk"
   },
   "source": [
    " ### Задача 1 (1 балл). Кроссвалидация, LOO, k-fold.\n",
    " \n",
    " Объясните, стоит ли использовать оценку `leave-one-out-CV` или `k-fold-CV` с небольшим $k$ в случае, когда:\n",
    "\n",
    " * обучающая выборка содержит очень малое количество объектов;\n",
    " \n",
    " Стоит использовать `leave-one-out`, так как основной его недостаток --- долгое вычисление на больших выборках.\n",
    " * обучающая выборка содержит очень большое количество объектов.\n",
    " \n",
    " Стоит использовать `k-fold-CV`, так как быстрее всего посчитает."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OK**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZX6sPkky9jS6"
   },
   "source": [
    "### Задача 2 (1 балл). Логистическая регрессия, вывод функции потерь.\n",
    "\n",
    "Рассмотрим выборку объектов $X = \\{x_1,...,x_\\ell\\}$ и их целевых меток $Y = \\{y_1,...,y_\\ell\\}$, где $y_i ∈ \\{0, 1\\}$. Предположим, что мы хотим обучить линейный классификатор:\n",
    "\n",
    "$$Q(w, X^\\ell) = \\sum\\limits_{i=1}^\\ell\\mathcal{L}(y_i\\left<w,x_i\\right>) \\rightarrow \\min\\limits_w$$\n",
    "\n",
    "где $w$ – веса линейной модели, $\\mathcal{L}(y, z)$ – некоторая гладкая функция потерь.\n",
    "\n",
    "Так как решается задача двухклассовой классификации, то будем обучать классификатор предсказывать вероятности принадлежности объекта классу $1$, то есть решать задачу логистической регрессии. Для измерения качества такого классификатора обычно используют правдоподобие $P(Y|X)$ целевых меток $Y$ при заданных объектах $X$ в соответствии с предсказанными распределениями $p$ — чем выше правдоподобие, тем точнее классификатор. Для удобства с вычислительной точки зрения обычно используется отрицательный логарифм правдоподобия, также называемый LogLoss (Logarithmic Loss). Будем считать, что пары объект-ответ $(x_i,y_i)$ независимы между собой для разных $i$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_UX9noDn-jcv"
   },
   "source": [
    "#### 1. (0.5 балла) \n",
    "Покажите что:\n",
    "\n",
    "$$\\text{LogLoss} =  -\\text{LogLikelihood} = - \\log(P(Y|X)) = - \\sum\\limits_{i=1}^\\ell (y_i \\log \\tilde y_i + (1-y_i) \\log (1-\\tilde y_i))$$\n",
    "\n",
    "#####  Решение:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\\begin{align}P(Y|X) &= \\prod_{i = 1}^n P(y_i|X) = \\prod_{i = 1}^n  \\begin{cases} \\tilde{y}_i, & y_i = 1\\\\ 1 - \\tilde{y}_i, & y_i = 0 \\end{cases}=\\\\\n",
    "&=\\prod_{i=1}^n \\widetilde y_i^{y_i} (1-\\widetilde y_i)^{1-y_i}.\n",
    "\\end{align}\n",
    "После взятия логарифма равенство становится очевидным."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OK**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sAZJvhZVA5-7"
   },
   "source": [
    "#### 2. (0.5 балла) \n",
    "Для того, чтобы классификатор возвращал числа из отрезка $[0,1]$, положите\n",
    "$$\n",
    "p(y_i=1|x_i) = \\sigma \\left ( \\left< w, x_i \\right> \\right) =\n",
    "\\frac{1}{1 + \\exp{\\left(- \\left< w, x_i \\right> \\right)}};\n",
    "$$\n",
    "\n",
    "сигмоидная функция монотонно возрастает, поэтому чем больше скалярное призведение, тем большая вероятность положительного класса будет предсказана объекту.\n",
    "\n",
    "Подставьте трансформированный ответ линейной модели в логарифм правдоподобия. К какой функции потерь мы пришли? (Обратите внимание, что функция обычно записывается для классов $\\{-1, 1\\}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Выходил к доске**, ответ\n",
    "\n",
    "$$\\text{LogLoss}=-\\sum \\log(1+\\exp(y_i\\langle w, x\\rangle)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lG4d_zxADKDF"
   },
   "source": [
    "### Задача 3 (1.5 балла). Логистическая регрессия, решение оптимизационной задачи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lA5GvcRcDY52"
   },
   "source": [
    "#### 1. (0.8 балла)\n",
    "\n",
    "Докажите, что в случае линейно разделимой выборки не существует вектора параметров (весов), который бы максимизировал правдоподобие вероятностной модели логистической регрессии в задаче двухклассовой классификации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Решение.\n",
    "\n",
    "Если $w$ максимум правдоподобия, то для любого $ i $ \n",
    "$$P(y_i|X)>0.5.$$\n",
    "Вероятность у нас вычисляется по формуле \n",
    "$$P(y_i|X)=\\sigma(y_i \\langle w, x_i\\rangle).$$\n",
    "Раз вероятность больше 0.5, то аргумент у сигмы положительный. Так что домножая вектор весов на число, большее 1, аргумент станет больше для любого $ i $, увеличивая сигму, а значит и правдоподобие. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3VBN5jiVE32G"
   },
   "source": [
    "#### 2. (0.4 балла)\n",
    "\n",
    "Предложите, как можно модифицировать вероятностную модель, чтобы оптимум достигался."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Решение:** добавим условие: вектор весов по модулю равен 1. Тогда если $w$ --- вектор ортогональный разделяющей плоскости, то существует единственный коэффициент $ с > 0$ такой, что $cw$ можно использовать как вектор весов, так что оптимум будет достигаться в нем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7C4UiKK3HV94"
   },
   "source": [
    "#### 3. (0.3 балла)\n",
    "Выпишите формулы пересчета значений параметров при оптимизации методом градиентного спуска для обычной модели логистической регрессии и предложенной модификации.\n",
    "\n",
    "Для 1:\n",
    "\n",
    "$$w\\mapsto w + \\frac 1t \\sum_{i=1}^l y_i\\sigma(-y_i\\langle w, x_i\\rangle) \\cdot x_i.$$\n",
    "\n",
    "Для 2:\n",
    "\n",
    "\n",
    "$$ w\\mapsto \\frac{w + \\frac 1t 2\\sum_{i=1}^l y_i\\sigma(a + \\langle w, x_i\\rangle) \\cdot  x_i}{||w + \\frac 1t 2\\sum_{i=1}^l y_i\\sigma(a + \\langle w, x_i\\rangle) \\cdot  x_i||}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NtrnMSYXKHRS"
   },
   "source": [
    "### Задача 4 (1 балл). Мультиномиальная регрессия\n",
    "\n",
    "В случае многоклассовой классификации логистическую регрессию можно обобщить: пусть для каждого класса $k$ есть свой вектор весов $w_k$. Тогда вероятность принадлежности классу $k$ запишем следующим образом:\n",
    "\n",
    "$$\n",
    "P(y=k | x, W) = \\frac{e^{\\langle w_k, x\\rangle}}{\\sum\\limits_{j=1}^K e^{\\langle w_j, x\\rangle}}\n",
    "$$\n",
    "\n",
    "Тогда оптимизируемая функция примет вид:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{sm}(W) = -\\sum_{i=1}^N \\sum_{k=1}^K [y_i=k]\\ln P(y_i=k | x_i, W),~\\text{где}~[y_i=k]=\\begin{cases}\n",
    "1, y_i=k,\\\\\n",
    "0, \\text{иначе}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Пусть количество классов $K=2$. Для простоты положим, что выборка линейно неразделима."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_Rd-dJWKm8r"
   },
   "source": [
    "#### 1. (0.5 балла)\n",
    " Единственно ли решение задачи? Почему? \n",
    " \n",
    " Если $w_1,w_2$ --- решения, то $w_1+w,w_2+w$ так же решения для любого вектора $w$, ибо $P(y-k|x,W)$ не поменяется, ибо числитель и каждое слагаемое в знаменателе домножатся на $ e^{\\langle w, x \\rangle} $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TXe4eM0JL0xL"
   },
   "source": [
    "#### 2. (0.5 балла)\n",
    " Покажите, что предсказанные распределения вероятностей на классах в случае логистической и мультиномиальной регрессий будут совпадать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очевидно из определения правдоподобия, что $P(y=k|x, w_1, w_2) = P(y=k|x, 0, w_2-w_1)$.\n",
    "\n",
    "Следовательно, $ \\mathcal{L}_{sm}(w_1,w_2) = \\mathcal{L}_{sm}(0,w_2 - w_1)$.\n",
    "Более того, несложно видеть, что \n",
    "$$ \\mathcal{L}_{sm}(0,w_2 - w_1) = LogLoss(w_2-w_1) $$, так что на самом деле можно считать, что в мультиномиальной регрессии один параметр, в котором значение правдоподобия совпадает с правдоподобией логистической регрессии, иными словами оптимизируется одна и та же функция."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OK**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2a3lm4KMFGl"
   },
   "source": [
    "### Задача 5 (1.5 балла) Решающие деревья, константное предсказание, функции потерь.\n",
    "\n",
    "Допустим, при построении решающего дерева в некоторый лист попало $N$ объектов $x_1, ... , x_N$ с метками $y_1, ... , y_N$.\n",
    "Предсказание в каждом листе дерева - константа. Найдите, какое значение $\\tilde y$\n",
    "должен предсказывать этот лист для минимизации следующих функций потерь:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBcO-ZNpMSYY"
   },
   "source": [
    "#### 1. (0.5 балла)\n",
    "Mean Squared Error (средний квадрат ошибки) для задачи регрессии:\n",
    "        \\begin{equation}Q=\\frac{1}{N}\\sum_{i=1}^N (y_i - \\tilde y)^2;\\end{equation}\n",
    "        \n",
    "$\\tilde y = \\frac 1N \\sum_{i=1}^N y_i$, так как известно, что для любой случайной величины $ X $ минимальное значение функции $\\mathbb{E}(X-t)^2$ достигается в точке $t = \\mathbb{E}X$ и значение является дисперсией.\n",
    "\n",
    "**ОК**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xrKaxpXmNaMf"
   },
   "source": [
    "#### 2. (0.5 балла) \n",
    " Mean Absolute Error (средний модуль отклонения) для задачи регрессии:\n",
    "        \\begin{equation}Q=\\frac{1}{N}\\sum_{i=1}^N |y_i - \\tilde y|.\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим $y_k \\leq \\tilde y = y_k + x \\leq y_{k+1}$. Тогда \n",
    "$$ Q(\\tilde y) = Q(y_k) + xk - x(N-k) = Q(y_k) + x(2k-N). $$\n",
    "Ошибка убывает с ростом $x$ при $k<\\frac N2$ и растет при больших $k$. Отмечу, что если $N$ четно, то при $k=N/2$ величина $x$ не влияет на ошибку. Таким образом $\\tilde y$ это просто напросто медиана."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ОК**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TnKGc3keOgF4"
   },
   "source": [
    "#### 3. (0.5 балла)\n",
    " $\\text{LogLoss}$ (логарифмические потери) для задачи классификации:\n",
    "        $$Q=-\\frac{1}{N}\\sum_{i=1}^N \\left(y_i\\log\\tilde y+(1-y_i)\\log(1-\\tilde y)\\right),\n",
    "            \\quad \\tilde y\\in[0,1], \\quad y_i \\in \\{0,1\\}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$0=Q' = -\\frac 1N \\sum_{i=1}^n \\frac{y_i}{\\tilde y} - \\frac{1-y_i}{1-\\tilde y}=-\\sum_{i=1}^n\\frac{\\bar y - \\tilde y}{\\tilde y (1-\\tilde y)},$$\n",
    "где $ \\bar y = \\frac 1N \\sum_{i=1}^n y_i. $ Тогда ясно, что минимум достигается в выборочном среднем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ОК**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cSmKQAILP0T0"
   },
   "source": [
    " ### Задача 6 (1 балл). Решающие деревья, функции потерь, impurity functions.\n",
    "$$\n",
    " \\Phi(U) - \\frac{|U_1|}{|U|}\\Phi(U_1) - \\frac{|U_2|}{|U|}\\Phi(U_2) \\to \\max\n",
    "$$\n",
    "таким выражением в лекции задается критерий, по которому происходит ветвление вершины  решающего дерева. Давайте разберемся подробнее. \n",
    "\n",
    "Impurity function $\\Phi(U)$ («функция неопределенности» или «функция нечистоты») используется для того, чтобы измерить степень неоднородности целевых меток $y_1,\\dots, y_\\ell$ для множества объектов $U$ размера $\\ell$. Например, при обучении решающего дерева в текущем листе  выбирается такое разбиение множества объектов $U$ на два непересекающихся множества $U_1$ и $U_2$, чтобы impurity function $\\Phi(U)$ исходного множества $U$ как можно сильнее превосходила нормированную impurity function в новых листьях $\\frac{|U_1|}{|U|}\\Phi(U_1) + \\frac{|U_2|}{|U|}\\Phi(U_2)$. Отсюда и получается, что нужно выбрать разбиение, решающее  задачу\n",
    "$$\n",
    " \\Phi(U) - \\frac{|U_1|}{|U|}\\Phi(U_1) - \\frac{|U_2|}{|U|}\\Phi(U_2) \\to \\max.\n",
    "$$\n",
    "Полученную разность называют Gain (выигрыш), и она показывает, на сколько удалось уменьшить «неопределенность» от разбиения листа два новых. \n",
    "\n",
    "В соответствии с одним из возможных определений, impurity function — это значение функционала ошибки $Q = \\frac{1}{\\ell}\\sum\\limits_{i=1}^\\ell \\mathcal{L}(y_i, \\tilde{y})$ в листе с множеством объектов $U$ при константном предсказании $\\tilde{y}$, оптимальном для $Q$ (см. задачу 7):\n",
    "    $$\n",
    "        \\Phi(U) = \\frac{1}{\\ell}\\sum\\limits_{i=1}^\\ell \\mathcal L (y_i, \\tilde y).\n",
    "    $$\n",
    "Понятно, что каждому критерию разбиения соответствует своя impurity function $\\Phi(U)$, а в основе каждой $\\Phi(U)$ лежит некоторая функция потерь. Давайте разберемся, откуда берутся различные критерии разбиения. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ZNypaDVQDYZ"
   },
   "source": [
    "#### 1. (0.5 балла)\n",
    "Покажите, что для квадратичных потерь $\\mathcal L (y_i, \\tilde y) = (y_i - \\tilde y)^2$ в задаче регрессии $y_i \\in \\mathbb{R}$ impurity function $\\Phi(U)$ равна выборочной дисперсии целевых меток объектов, попавших в лист дерева."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как было показано в 5.1, при  квадратичных потерях $\\mathcal{L}(y_i,i)=(y_i-y)^2$ $\\tilde y = \\bar y$ --- выборочному среднему. Таким образом, \n",
    "$$ \\Phi(U) = \\frac 1l \\sum_{i=1}^l \\mathcal L (y_i,\\tilde y) = \\frac 1l \\sum_{i=1}^l (y_i-\\bar y)^2 = S(y),$$\n",
    "она же выборочная дисперсия.\n",
    "\n",
    "**ОК**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MeWp8iIxR96A"
   },
   "source": [
    "#### 2. (0.5 балла)\n",
    " Покажите, что для функции потерь $\\text{Logloss}$  $\\mathcal L (y_i, \\tilde y) =-y_i\\log(\\tilde y) - (1-y_i)\\log(1 - \\tilde y)$ в задаче классификации $y_i \\in \\{0,1\\}$ impurity function $\\Phi(U)$ соответствует энтропийному критерию разбиения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Энтропийный критерий разбиения\n",
    "$$\\Phi(U) = -q\\log_2 q - (1-q) \\log_2 (1-q),$$\n",
    "где $q = P(y=1|U)$, иными словами доля 1 среди значений $ y $ при данных значениях $U$. Однако $ \\bar y $  так же является таковым, ибо это количество значений 1 деленное на количество значений. Осталось заметить, что\n",
    "$$\\Phi(U) = \\text{LogLoss} \\mathcal L (y_i, \\tilde y) =[\\text{см 5.3}]= \\mathcal L (y_i, \\bar y) = \\mathcal L (y_i, q).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ОК**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KvAw2PMSTrkw"
   },
   "source": [
    "### Задача 7 (1 балл). Решающие деревья, индекс Джини\n",
    "\n",
    "Пусть имеется построенное решающее дерево для задачи многоклассовой классификации. Рассмотрим лист дерева с номером $m$ и объекты $R_m$, попавшие в него. Обозначим за $p_{mk}$ долю объектов $k$-го класса в листе $m$. *Индексом Джини* этого листа называется величина\n",
    "$$\\sum_{k = 1}^{K} p_{mk} (1 - p_{mk}),$$\n",
    "где $K$ — общее количество классов. Индекс Джини обычно служит мерой того, насколько хорошо в данном листе выделен какой-то один класс (см. impurity function в предыдущей задаче)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "39o2PqSqT9U-"
   },
   "source": [
    "#### 1. (0.5 балла)\n",
    "Поставим в соответствие листу $m$ алгоритм классификации $a(x)$, который предсказывает класс случайно, причем класс $k$ выбирается с вероятностью $p_{mk}$. Покажите, что матожидание частоты ошибок этого алгоритма на объектах из $R_m$ равно индексу Джини. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Частота ошибок --- это вероятность не угадать класс, которая вычисляется следующим образом:\n",
    "$$\\sum_{i=1}^K P(y_{predicted} = i, y_{actual}\\ne i) = \\sum_{i=1}^K p_{mk} \\cdot (1-p_{mk}),$$\n",
    "где события в левой части несовместны, оттого и сумма ($y_{predicted}$ не может быть одновременно 1 и 2, например), а события внутри P для любого i независимы, ибо наше предсказание не зависит от реального значения(иначе не понятно что такое случайное предсказание), следовательно вероятности перемножаются."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2_WiDXWU5Mu"
   },
   "source": [
    "#### 2. (0.5 балла)\n",
    "*Дисперсией класса $k$* назовем дисперсию выборки $\\{ [y_i = k]:\\ x_i \\in R_m$\\},\n",
    "    где $y_i$ - класс объекта $x_i$, $[f]$ — индикатор истинности выражения $f$, равный 1 если $f$ верно, и нулю в противном случае, а $R_m$ — множество объектов в листе.\n",
    "    Покажите, что сумма дисперсий всех классов в заданном листе равна его индексу Джини."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дисперсия класса $k$ является дисперсией Бернуллевской выборки $\\{ [y_i = k]:\\ x_i \\in R_m\\}$, где доля 1 равна $p_{mk}$. Известный факт, что дисперсия у Бернуллевской величины равна $p_{mk}(1-p_{mk})$. Далее берется сумма всех дисперсий, что натурально является индексом Джини."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ок**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6-ynDEVYCF4"
   },
   "source": [
    "### Задача 8 (2 балла). Бинарные решающие деревья, MSE\n",
    "\n",
    "Предложите алгоритм построения *оптимального* бинарного решающего дерева для задачи регрессии на $\\ell$ объектах в $n$-мерном пространстве с асимптотической сложностью $O(n \\ell \\log \\ell)$. В качестве предикатов нужно рассматривать пороговые правила (наиболее распространенный случай на практике). Для простоты можно считать, что  получающееся дерево близко к сбалансированному (т.е. его глубина имеет порядок $O(\\log \\ell)$) и в качестве функции ошибки  используется Mean Squared Error (MSE):\n",
    "\t$$Q=\\frac{1}{\\ell}\\sum_{i=1}^\\ell (y_i - \\tilde y_i)^2$$\n",
    "Под оптимальностью в данной задаче подразумевается, что в каждом узле дерева делается оптимальное с точки зрения MSE разбиение на два поддерева.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На каждом уровне узлы имеют суммарный размер $\\leq l$, так что если каждый узел будет $v$ будет обрабатываться за $O(|v|n)$ действий, то алгоритм будет работать за $O(depth \\cdot nl )=O( l\\log nl)$, что и требуется. Алгоритм будет следующий:\n",
    "* Перебираем признаки.\n",
    "* В каждом перебираем пороги.\n",
    "* Заранее считаем суммы на префиксах и суффиксах (если их делить на индекс префикса +1, то получится $\\tilde y_i$ при сплите на соответствующем пороге).\n",
    "* Для первого порога честно считаем изменение MSE.\n",
    "* Для каждого следующего используя массив префиксных и суффиксных сумм можно за O(1) посчитать изменение MSE:\n",
    "$$ Q = \\frac 1{l+1}\\sum_{i=1}^{l+1}(y_i-\\tilde y_i)^2 =[\\delta = \\tilde y_i - \\tilde y_{i,prev}]= \\frac 1{l+1}\\sum_{i=1}^{l+1}(y_i-\\tilde y_{i,prev} - \\delta)^2 = $$\n",
    "$$ =\\frac l{l+1}Q_{prev} -\\frac 1{l+1}\\sum_{i=1}^l 2\\delta (y_i-\\tilde y_{i,prev}) - \\delta^2+\\frac 1{l+1}(y_{l+1}-\\tilde y_{l+1})^2.$$\n",
    "Учитывая предподсчет префиксов и суффиксов, за $|v|$ смогли найти наилучший сплит по признаку $k$ для любого $k=1,\\dots,n$"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Practice5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
